{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCJvPsD0g-Sh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import matplotlib.transforms as mtransforms\n",
        "import matplotlib.ticker as mticker\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "import shap\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# CONFIGURATION\n",
        "# ========================================\n",
        "USE_EXTERNAL_VALIDATION = True  # True: run external validation | False: skip validation\n",
        "SAVE_MODEL = True  # True: save model to .pkl file | False: skip saving"
      ],
      "metadata": {
        "id": "x3bajDqp6XI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # For Colab: upload file\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    filename = next(iter(uploaded))\n",
        "    df = pd.read_csv(filename)\n",
        "except:\n",
        "    # For local execution: use predefined path\n",
        "    df = pd.read_csv('../data/training/Oliynyk_Js_optunaFS.csv')\n",
        "\n",
        "print(f\"Loaded {len(df)} samples with {len(df.columns)} columns\")\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "VhYZp2d46Z1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_EXTERNAL_VALIDATION:\n",
        "        # Configuration: Set target property and feature type\n",
        "    property_name = \"Js\"  # \"Hc\", \"Js\", or \"rho\"\n",
        "    featurization_method = \"Oliynyk\"  # \"Composition\", \"WenAlloys\", or \"Oliynyk\"\n",
        "    output_file = f\"predictions_{property_name}_{featurization_method}.xlsx\"\n",
        "    column_name = f\"{property_name}_predict\"\n",
        "\n",
        "    # Load experimental dataset\n",
        "    try:\n",
        "        # Colab: upload file\n",
        "        uploaded = files.upload()\n",
        "        X_predict = pd.read_csv('oliynyk_Js_for_predict.csv')\n",
        "    except:\n",
        "        # Local: load from directory\n",
        "        X_predict = pd.read_csv('../data/external_validation/oliynyk_Js_for_predict.csv')\n",
        "\n",
        "    print(f\"Loaded {len(X_predict)} experimental samples with {len(X_predict.columns)} features\")\n",
        "    display(X_predict.head())\n",
        "\n",
        "    # Initialize prediction results table (5 alloys × 2 models)\n",
        "    results = pd.DataFrame({\n",
        "        \"Model\": [\"ETR\"]*5 + [\"XGB\"]*5,\n",
        "        \"Alloy_number\": list(range(1, 6)) + list(range(1, 6)),\n",
        "        column_name: [np.nan]*10\n",
        "    })\n",
        "\n",
        "    print(f\"\\nPrediction table initialized for {property_name} ({featurization_method})\")\n",
        "    print(results, \"\\n\")"
      ],
      "metadata": {
        "id": "4QIl9h2G6kjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and target variable\n",
        "df_drop_column = df.iloc[:, :-1]  # All columns except the last one (features)\n",
        "y = df.loc[:, 'target']            # Target column only"
      ],
      "metadata": {
        "id": "d-CJdilK7aUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Training Dataset"
      ],
      "metadata": {
        "id": "sTacxSXcPPl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename features to abbreviated forms for better visualization\n",
        "df_renamed = df_drop_column.rename(columns={\n",
        "    'avg_l_quantum_number': 'avg q. num. l',\n",
        "    'avg_group': 'avg group',\n",
        "    'dev_Mendeleev_Number': 'Δ M. num.',\n",
        "    'avg_Zunger_radii_sum': 'avg rad. Zunger',\n",
        "    'sum_specific_heat_(J/g_K)_': 'Σ specific heat',\n",
        "    'avg_crystal_radius': 'avg cryst. rad.'\n",
        "})"
      ],
      "metadata": {
        "id": "Pz20go6a1uad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for training\n",
        "features = df_renamed\n",
        "target = y\n",
        "\n",
        "# Shuffle data to ensure random distribution\n",
        "features, target = shuffle(features, target, random_state=42)\n",
        "\n",
        "# Split into training and test sets (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "7SS6W-EaymM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty DataFrame to store all results\n",
        "model_results = pd.DataFrame(columns=['Model', 'MAE', 'MSE', 'RMSE', 'R²'])"
      ],
      "metadata": {
        "id": "YSUUfU3WK-Dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atrGb9OUbU70"
      },
      "source": [
        "# Prediction Properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyJJLHmOw9S0"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K-XrAfae5e_"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yQ4PpQbfmPh"
      },
      "outputs": [],
      "source": [
        "# Train Random Forest model with pre-optimized hyperparameters\n",
        "best_model = RandomForestRegressor(\n",
        "    n_estimators=270,\n",
        "    max_depth=10,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    criterion='friedman_mse',\n",
        "    random_state=42\n",
        ")\n",
        "best_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if SAVE_MODEL:\n",
        "    # Save trained model\n",
        "    joblib.dump(best_model, '1_oliynyk_SP_rfr.pkl')\n",
        "    print(\"✅ Model saved as '1_oliynyk_SP_rfr.pkl'\")\n",
        "\n",
        "    try:\n",
        "        files.download('1_oliynyk_SP_rfr.pkl')\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "n9bDmaaO9SdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjbPMNXZfq6V"
      },
      "outputs": [],
      "source": [
        "# Predict on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "test_r2 = r2_score(y_test, y_pred)\n",
        "test_mae = mean_absolute_error(y_test, y_pred)\n",
        "test_mse = mean_squared_error(y_test, y_pred)\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "\n",
        "# Display metrics in formatted table\n",
        "print(\"\\nTest Set Evaluation Metrics:\")\n",
        "print(\"----------------------------------------\")\n",
        "print(f\"| {'Metric':<10} | {'Value':>15} |\")\n",
        "print(\"|------------|----------------|\")\n",
        "print(f\"| R²         | {test_r2:>15.4f} |\")\n",
        "print(f\"| MAE        | {test_mae:>15.4f} |\")\n",
        "print(f\"| MSE        | {test_mse:>15.4f} |\")\n",
        "print(f\"| RMSE       | {test_rmse:>15.4f} |\")\n",
        "print(\"----------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaIP9xkAv3gl"
      },
      "outputs": [],
      "source": [
        "def evaluate_and_store(model, model_name, X_test, y_test, results_df):\n",
        "    \"\"\"Evaluate model and store metrics in results DataFrame.\"\"\"\n",
        "\n",
        "    # Predict and calculate metrics\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'MAE': mean_absolute_error(y_test, y_pred),\n",
        "        'MSE': mean_squared_error(y_test, y_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "        'R²': r2_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    # Add to results\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([metrics])], ignore_index=True)\n",
        "    return results_df\n",
        "\n",
        "\n",
        "# Store RFR evaluation results\n",
        "model_results = evaluate_and_store(best_model, 'RFR', X_test, y_test, model_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVpvv6W7mf03"
      },
      "source": [
        "# Extra Trees Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2xkaSVJn4Z9"
      },
      "outputs": [],
      "source": [
        "# Train Extra Trees model with pre-optimized hyperparameters\n",
        "best_model = ExtraTreesRegressor(\n",
        "    n_estimators=270,\n",
        "    max_depth=7,\n",
        "    min_samples_split=4,\n",
        "    min_samples_leaf=2,\n",
        "    criterion='poisson',\n",
        "    random_state=42\n",
        ")\n",
        "best_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if SAVE_MODEL:\n",
        "\n",
        "# Save model to .pkl file\n",
        "    joblib.dump(best_model, '2_oliynyk_SP_etr.pkl')\n",
        "\n",
        "    print(\"✅ Model saved as '2_oliynyk_SP_etr.pkl'\")\n",
        "    try:\n",
        "        files.download('2_oliynyk_SP_etr.pkl')\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "q0HAXXu69vUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MfF7yTBn7f2"
      },
      "outputs": [],
      "source": [
        "# Predict on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "test_r2 = r2_score(y_test, y_pred)\n",
        "test_mae = mean_absolute_error(y_test, y_pred)\n",
        "test_mse = mean_squared_error(y_test, y_pred)\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "\n",
        "# Display metrics in formatted table\n",
        "print(\"\\nTest Set Evaluation Metrics:\")\n",
        "print(\"----------------------------------------\")\n",
        "print(f\"| {'Metric':<10} | {'Value':>15} |\")\n",
        "print(\"|------------|----------------|\")\n",
        "print(f\"| R²         | {test_r2:>15.4f} |\")\n",
        "print(f\"| MAE        | {test_mae:>15.4f} |\")\n",
        "print(f\"| MSE        | {test_mse:>15.4f} |\")\n",
        "print(f\"| RMSE       | {test_rmse:>15.4f} |\")\n",
        "print(\"----------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_EXTERNAL_VALIDATION:\n",
        "    # Make predictions on external validation set (ETR)\n",
        "    y_pred_etr = best_model.predict(X_predict)\n",
        "\n",
        "    # Save ETR predictions to results DataFrame\n",
        "    results.loc[results[\"Model\"] == \"ETR\", column_name] = y_pred_etr\n",
        "    print(f\"ETR predictions saved: {y_pred_etr}\")\n",
        "    print(results)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "v8oDq1h593aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YilHJfefw03S"
      },
      "outputs": [],
      "source": [
        "def evaluate_and_store(model, model_name, X_test, y_test, results_df):\n",
        "    \"\"\"Evaluate model and store metrics in results DataFrame.\"\"\"\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate regression metrics\n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'MAE': mean_absolute_error(y_test, y_pred),\n",
        "        'MSE': mean_squared_error(y_test, y_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "        'R²': r2_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    # Append metrics to results DataFrame\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([metrics])], ignore_index=True)\n",
        "    return results_df\n",
        "\n",
        "\n",
        "# Evaluate ETR model and store results\n",
        "model_results = evaluate_and_store(best_model, 'ETR', X_test, y_test, model_results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================\n",
        "# SHAP BEESWARM VISUALIZATION\n",
        "# ====================================================================\n",
        "\n",
        "# Calculate SHAP values for all features using TreeExplainer\n",
        "explainer = shap.TreeExplainer(best_model, X_train)\n",
        "shap_values = explainer(X_train)\n",
        "\n",
        "\n",
        "# Set global font parameters\n",
        "mpl.rcParams.update({\n",
        "    'font.size': 23,\n",
        "    'axes.labelsize': 23,\n",
        "    'xtick.labelsize': 23,\n",
        "    'ytick.labelsize': 23,\n",
        "})\n",
        "\n",
        "\n",
        "# Create figure and axis\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "\n",
        "# Generate beeswarm plot showing feature importance and value distribution\n",
        "shap.plots.beeswarm(\n",
        "    shap_values,\n",
        "    max_display=5,                     # Show top 5 most important features\n",
        "    group_remaining_features=False,    # Don't group remaining features\n",
        "    show=False,                        # Don't display yet (for customization)\n",
        "    s=150,                             # Marker size\n",
        "    ax=ax,\n",
        "    plot_size=None\n",
        ")\n",
        "\n",
        "\n",
        "# Customize axis labels and ticks\n",
        "ax.set_xlabel(\"SHAP Values\", fontsize=23)\n",
        "ax.tick_params(axis='x', labelsize=23, pad=2)\n",
        "ax.tick_params(axis='y', which='both', length=0, pad=1, labelsize=23)\n",
        "\n",
        "\n",
        "# Set custom x-axis tick positions\n",
        "desired_ticks = [-0.4, -0.2, 0, 0.2]\n",
        "ax.set_xticks(desired_ticks)\n",
        "\n",
        "\n",
        "# Shift y-axis labels to the right\n",
        "dx_pt = -3\n",
        "offset = mtransforms.ScaledTranslation(dx_pt/72.0, 0, fig.dpi_scale_trans)\n",
        "for t in ax.get_yticklabels():\n",
        "    t.set_ha('right')\n",
        "    t.set_transform(t.get_transform() + offset)\n",
        "\n",
        "\n",
        "# Position y-axis spine\n",
        "ax.spines['left'].set_position(('axes', 0.0))\n",
        "ax.yaxis.set_ticks_position('left')\n",
        "\n",
        "\n",
        "# Customize colorbar (feature value scale)\n",
        "cax = fig.axes[-1]\n",
        "cax.tick_params(pad=2, labelsize=23)\n",
        "cax.set_ylabel(\"Feature Value\", fontsize=23, labelpad=4)\n",
        "cax.yaxis.set_label_coords(2.5, 0.5)\n",
        "\n",
        "\n",
        "# Apply tight layout and save\n",
        "fig.tight_layout(pad=0.2)\n",
        "fig.savefig('Oliynyk_SHAP_beeswarm_Js_ETR.png', dpi=600, bbox_inches='tight', transparent=False)\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Download plot in Colab environment\n",
        "try:\n",
        "    files.download('Oliynyk_SHAP_beeswarm_Js_ETR.png')\n",
        "except:\n",
        "    pass  # Skip download for local execution"
      ],
      "metadata": {
        "id": "tjqj3HFx_i27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shifted_coolwarm_cmap(p, n=256):\n",
        "    \"\"\"Shift coolwarm colormap so position p becomes neutral center.\"\"\"\n",
        "    base = plt.get_cmap(\"coolwarm\")\n",
        "    xs = np.linspace(0, 1, n)\n",
        "\n",
        "    cols = []\n",
        "    for x in xs:\n",
        "        if x <= p:\n",
        "            t = 0.5 * (x / p) if p > 0 else 0.0\n",
        "        else:\n",
        "            t = 0.5 + 0.5 * ((x - p) / (1 - p)) if p < 1 else 1.0\n",
        "        cols.append(base(t))\n",
        "\n",
        "    return mcolors.ListedColormap(cols, name=\"coolwarm_shifted\")\n",
        "\n",
        "\n",
        "def make_coolwarm_saturated_with_center(values, vcenter=0.0, nbins=6, steps=(1, 2, 2.5, 3, 5, 10)):\n",
        "    \"\"\"Create norm, cmap, and ticks for SHAP with saturated colors and zero at neutral.\"\"\"\n",
        "    x = np.asarray(values, dtype=float)\n",
        "    x = x[np.isfinite(x)]\n",
        "\n",
        "    # Fallback for empty data\n",
        "    if x.size == 0:\n",
        "        norm = mcolors.Normalize(vmin=-1, vmax=1, clip=True)\n",
        "        cmap = plt.get_cmap(\"coolwarm\")\n",
        "        ticks = np.array([-1, 0, 1], dtype=float)\n",
        "        return norm, cmap, ticks\n",
        "\n",
        "    data_min, data_max = float(np.min(x)), float(np.max(x))\n",
        "\n",
        "    # Normalize by actual data range for saturated colors\n",
        "    norm = mcolors.Normalize(vmin=data_min, vmax=data_max, clip=True)\n",
        "\n",
        "    # Generate ticks\n",
        "    loc = mticker.MaxNLocator(nbins=nbins, steps=list(steps))\n",
        "    ticks = loc.tick_values(data_min, data_max)\n",
        "    ticks = ticks[(ticks >= norm.vmin) & (ticks <= norm.vmax)]\n",
        "\n",
        "    # Shift colormap to place vcenter at neutral\n",
        "    if data_min < vcenter < data_max:\n",
        "        p = (vcenter - data_min) / (data_max - data_min)\n",
        "        cmap = shifted_coolwarm_cmap(p, n=256)\n",
        "\n",
        "        # Ensure vcenter is in ticks\n",
        "        if not np.any(np.isclose(ticks, vcenter)):\n",
        "            ticks = np.sort(np.append(ticks, vcenter))\n",
        "    else:\n",
        "        cmap = plt.get_cmap(\"coolwarm\")\n",
        "\n",
        "    return norm, cmap, ticks"
      ],
      "metadata": {
        "id": "AbzXGk6KAGTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================\n",
        "# GRID-SHAP VISUALIZATION\n",
        "# ====================================================================\n",
        "\n",
        "# Calculate SHAP values using TreeExplainer\n",
        "explainer = shap.TreeExplainer(best_model, X_train)\n",
        "shap_values = explainer(X_train)\n",
        "\n",
        "# Identify top 5 most important features by mean absolute SHAP value\n",
        "mean_abs_shap_values = np.abs(shap_values.values).mean(axis=0)\n",
        "top5_indices = np.argsort(mean_abs_shap_values)[-5:][::-1]\n",
        "\n",
        "# Create figure with 3x2 grid layout\n",
        "plt.style.use('default')\n",
        "fig = plt.figure(figsize=(8, 10))\n",
        "gs = GridSpec(3, 2, figure=fig, wspace=0.65, hspace=0.5)\n",
        "\n",
        "# Generate scatter plots for each top feature\n",
        "for i, feature_index in enumerate(top5_indices):\n",
        "    feature_name = X_train.columns[feature_index]\n",
        "    row = i // 2\n",
        "    col = i % 2\n",
        "\n",
        "    ax = fig.add_subplot(gs[row, col])\n",
        "    ax.set_facecolor('#f0f0f0')\n",
        "\n",
        "    # Extract SHAP values for current feature\n",
        "    shap_col = shap_values[:, feature_index].values\n",
        "\n",
        "    # Get normalized colormap with zero at neutral center\n",
        "    norm, cmap_local, ticks = make_coolwarm_saturated_with_center(\n",
        "        shap_col,\n",
        "        vcenter=0.0,\n",
        "        nbins=6\n",
        "    )\n",
        "\n",
        "    # Create scatter plot: feature value vs target, colored by SHAP\n",
        "    sc = ax.scatter(\n",
        "        X_train[feature_name],\n",
        "        y_train,\n",
        "        c=shap_col,\n",
        "        cmap=cmap_local,\n",
        "        norm=norm,\n",
        "        alpha=0.9,\n",
        "        s=40,\n",
        "        edgecolor='black',\n",
        "        linewidth=0.25,\n",
        "        marker='o'\n",
        "    )\n",
        "\n",
        "    # Add colorbar with scientific notation\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "    cbar = fig.colorbar(sc, cax=cax)\n",
        "\n",
        "    # Configure colorbar ticks\n",
        "    cbar.set_ticks(ticks)\n",
        "    cbar.formatter = mticker.ScalarFormatter(useMathText=False)\n",
        "    cbar.formatter.set_powerlimits((-100, 100))\n",
        "    cbar.update_ticks()\n",
        "    cbar.ax.tick_params(labelsize=11)\n",
        "    cbar.set_label('SHAP Value', fontsize=12, labelpad=5)\n",
        "\n",
        "    # Configure axes\n",
        "    ax.yaxis.set_major_locator(mticker.MaxNLocator(min_n_ticks=4, nbins=5))\n",
        "    ax.xaxis.set_major_locator(mticker.MaxNLocator(min_n_ticks=3, nbins=5, integer=True))\n",
        "    ax.set_xlabel(feature_name, fontsize=12, labelpad=4)\n",
        "    ax.set_ylabel(\"Saturation Polarization (T)\", fontsize=12, labelpad=4)\n",
        "    ax.tick_params(axis='both', labelsize=11, pad=4)\n",
        "    ax.grid(True, linestyle=':', alpha=0.35, linewidth=0.45)\n",
        "\n",
        "# Save high-resolution figure\n",
        "plt.savefig('Oliynyk_Grid-SHAP_Js_ETR.png',\n",
        "            dpi=600,\n",
        "            bbox_inches='tight',\n",
        "            facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "# Download in Colab environment\n",
        "try:\n",
        "    files.download('Oliynyk_Grid-SHAP_Js_ETR.png')\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "-UGc4DgAAJHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KYoSztSflco"
      },
      "source": [
        "# Gradient Boost Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzhAd428EgK1"
      },
      "outputs": [],
      "source": [
        "# Train Gradient Boosting model with pre-optimized hyperparameters\n",
        "best_model = GradientBoostingRegressor(\n",
        "    n_estimators=180,\n",
        "    learning_rate=0.04,\n",
        "    max_depth=3,\n",
        "    min_samples_split=6,\n",
        "    min_samples_leaf=3,\n",
        "    subsample=0.64,\n",
        "    loss='squared_error',\n",
        "    random_state=42\n",
        ")\n",
        "best_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if SAVE_MODEL:\n",
        "\n",
        "# Save model to .pkl file\n",
        "    joblib.dump(best_model, '3_oliynyk_SP_gbr.pkl')\n",
        "\n",
        "    print(\"✅ Model saved as '3_oliynyk_SP_gbr.pkl'\")\n",
        "    try:\n",
        "        files.download('3_oliynyk_SP_gbr.pkl')\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "oj6W1KL3AT33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeqtdUwyEWr_"
      },
      "outputs": [],
      "source": [
        "# Predict on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "test_r2 = r2_score(y_test, y_pred)\n",
        "test_mae = mean_absolute_error(y_test, y_pred)\n",
        "test_mse = mean_squared_error(y_test, y_pred)\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "\n",
        "# Display metrics in formatted table\n",
        "print(\"\\nTest Set Evaluation Metrics:\")\n",
        "print(\"----------------------------------------\")\n",
        "print(f\"| {'Metric':<10} | {'Value':>15} |\")\n",
        "print(\"|------------|----------------|\")\n",
        "print(f\"| R²         | {test_r2:>15.4f} |\")\n",
        "print(f\"| MAE        | {test_mae:>15.4f} |\")\n",
        "print(f\"| MSE        | {test_mse:>15.4f} |\")\n",
        "print(f\"| RMSE       | {test_rmse:>15.4f} |\")\n",
        "print(\"----------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgTYWDUsxBLD"
      },
      "outputs": [],
      "source": [
        "def evaluate_and_store(model, model_name, X_test, y_test, results_df):\n",
        "    \"\"\"Evaluate model and store metrics in results DataFrame.\"\"\"\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate regression metrics\n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'MAE': mean_absolute_error(y_test, y_pred),\n",
        "        'MSE': mean_squared_error(y_test, y_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "        'R²': r2_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    # Append metrics to results DataFrame\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([metrics])], ignore_index=True)\n",
        "    return results_df\n",
        "\n",
        "\n",
        "# Evaluate GBR model and store results\n",
        "model_results = evaluate_and_store(best_model, 'GBR', X_test, y_test, model_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS9JUMRRr8g-"
      },
      "source": [
        "# XGBoost Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQl0rNV1uMBA"
      },
      "outputs": [],
      "source": [
        "# Train XGBoost model with pre-optimized hyperparameters\n",
        "mean_target = float(y_train.mean())\n",
        "best_model = xgb.XGBRegressor(\n",
        "    n_estimators=350,\n",
        "    max_depth=3,\n",
        "    learning_rate=0.15,\n",
        "    subsample=0.74,\n",
        "    colsample_bytree=0.88,\n",
        "    min_child_weight=1,\n",
        "    gamma=0.01,\n",
        "    alpha=0.01,\n",
        "    base_score=mean_target,\n",
        "    random_state=42\n",
        ")\n",
        "best_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if SAVE_MODEL:\n",
        "\n",
        "# Save model to .pkl file\n",
        "    joblib.dump(best_model, '4_oliynyk_SP_xgb.pkl')\n",
        "\n",
        "    print(\"✅ Model saved as '4_oliynyk_SP_xgb.pkl'\")\n",
        "    try:\n",
        "        files.download('4_oliynyk_SP_xgb.pkl')\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "XNWb-PpaAlO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsQH4ijDuQAA"
      },
      "outputs": [],
      "source": [
        "# Predict on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "test_r2 = r2_score(y_test, y_pred)\n",
        "test_mae = mean_absolute_error(y_test, y_pred)\n",
        "test_mse = mean_squared_error(y_test, y_pred)\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "\n",
        "# Display metrics in formatted table\n",
        "print(\"\\nTest Set Evaluation Metrics:\")\n",
        "print(\"----------------------------------------\")\n",
        "print(f\"| {'Metric':<10} | {'Value':>15} |\")\n",
        "print(\"|------------|----------------|\")\n",
        "print(f\"| R²         | {test_r2:>15.4f} |\")\n",
        "print(f\"| MAE        | {test_mae:>15.4f} |\")\n",
        "print(f\"| MSE        | {test_mse:>15.4f} |\")\n",
        "print(f\"| RMSE       | {test_rmse:>15.4f} |\")\n",
        "print(\"----------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_EXTERNAL_VALIDATION:\n",
        "    # Make predictions on external validation set (XGB)\n",
        "    y_pred_xgb = best_model.predict(X_predict)\n",
        "\n",
        "    # Save XGB predictions to results DataFrame\n",
        "    results.loc[results[\"Model\"] == \"XGB\", column_name] = y_pred_xgb\n",
        "    print(f\"XGB predictions saved: {y_pred_xgb}\")\n",
        "    print(results)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Add experimental ground truth values\n",
        "    y_true = [0.937, 0.884, 0.607, 0.794, 1.574]  # ← UPDATE WITH ACTUAL VALUES\n",
        "    results[\"y_true\"] = y_true * 2  # Duplicate for both models\n",
        "\n",
        "    # Calculate errors\n",
        "    results[\"error\"] = results[column_name] - results[\"y_true\"]\n",
        "    results[\"abs_error\"] = results[\"error\"].abs()\n",
        "    results[\"abs_percent_error_%\"] = 100 * results[\"abs_error\"] / results[\"y_true\"]\n",
        "\n",
        "    # Calculate average metrics per model\n",
        "    for model in [\"ETR\", \"XGB\"]:\n",
        "        mask = results[\"Model\"] == model\n",
        "        mae = results.loc[mask, \"abs_error\"].mean()\n",
        "        mape = results.loc[mask, \"abs_percent_error_%\"].mean()\n",
        "        print(f\"{model}: MAE = {mae:.4f}, MAPE = {mape:.2f}%\")\n",
        "\n",
        "    # Save results to Excel\n",
        "    results.to_excel(output_file, index=False, engine='openpyxl')\n",
        "    print(f\"✓ Results saved to {output_file}\")\n",
        "\n",
        "    # Download file (Colab only)\n",
        "    try:\n",
        "        files.download(output_file)\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "wCW4ZNk5Avyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVZfYUlwxJT3"
      },
      "outputs": [],
      "source": [
        "def evaluate_and_store(model, model_name, X_test, y_test, results_df):\n",
        "    \"\"\"\n",
        "    Evaluate model and store results in the results DataFrame\n",
        "    Returns updated DataFrame\n",
        "    \"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'MAE': mean_absolute_error(y_test, y_pred),\n",
        "        'MSE': mean_squared_error(y_test, y_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "        'R²': r2_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([metrics])], ignore_index=True)\n",
        "    return results_df\n",
        "\n",
        "model_results = evaluate_and_store(best_model, 'XGB', X_test, y_test, model_results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================\n",
        "# SHAP BEESWARM VISUALIZATION (XGBOOST)\n",
        "# ====================================================================\n",
        "\n",
        "# Create DMatrix for XGBoost native SHAP calculation\n",
        "dmatrix_train = xgb.DMatrix(X_train, feature_names=X_train.columns.tolist())\n",
        "\n",
        "# Calculate SHAP values using XGBoost native API\n",
        "shap_values_raw = best_model.get_booster().predict(dmatrix_train, pred_contribs=True)\n",
        "shap_values_matrix = shap_values_raw[:, :-1]  # Feature contributions\n",
        "expected_value = float(shap_values_raw[0, -1])  # Base value\n",
        "\n",
        "print(f\"Shape SHAP values: {shap_values_matrix.shape}\")\n",
        "print(f\"Expected value: {expected_value}\")\n",
        "\n",
        "# Create SHAP Explanation object\n",
        "explanation = shap.Explanation(\n",
        "    values=shap_values_matrix,\n",
        "    base_values=np.full(len(X_train), expected_value),\n",
        "    data=X_train.values,\n",
        "    feature_names=X_train.columns.tolist()\n",
        ")\n",
        "\n",
        "# Set global font parameters for publication-quality plots\n",
        "mpl.rcParams.update({\n",
        "    'font.size': 23,\n",
        "    'axes.labelsize': 23,\n",
        "    'xtick.labelsize': 23,\n",
        "    'ytick.labelsize': 23,\n",
        "})\n",
        "\n",
        "# Create figure and axis\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# Generate beeswarm plot showing feature importance and value distribution\n",
        "shap.plots.beeswarm(\n",
        "    explanation,\n",
        "    max_display=5,                     # Show top 5 most important features\n",
        "    group_remaining_features=False,    # Don't group remaining features\n",
        "    show=False,                        # Don't display yet (for customization)\n",
        "    s=150,                             # Marker size\n",
        "    ax=ax,\n",
        "    plot_size=None\n",
        ")\n",
        "\n",
        "# Customize axis labels and ticks\n",
        "ax.set_xlabel(\"SHAP Values\", fontsize=23)\n",
        "ax.tick_params(axis='x', labelsize=23, pad=2)\n",
        "ax.tick_params(axis='y', which='both', length=0, pad=1, labelsize=23)\n",
        "\n",
        "# Set custom x-axis tick positions\n",
        "# desired_ticks = [-0.6, -0.3, 0, 0.3]\n",
        "# ax.set_xticks(desired_ticks)\n",
        "\n",
        "# Shift y-axis labels to the right for better readability\n",
        "dx_pt = -3\n",
        "offset = mtransforms.ScaledTranslation(dx_pt/72.0, 0, fig.dpi_scale_trans)\n",
        "for t in ax.get_yticklabels():\n",
        "    t.set_ha('right')\n",
        "    t.set_transform(t.get_transform() + offset)\n",
        "\n",
        "# Position y-axis spine\n",
        "ax.spines['left'].set_position(('axes', 0.0))\n",
        "ax.yaxis.set_ticks_position('left')\n",
        "\n",
        "# Customize colorbar (feature value scale)\n",
        "cax = fig.axes[-1]\n",
        "cax.tick_params(pad=2, labelsize=23)\n",
        "cax.set_ylabel(\"Feature Value\", fontsize=23, labelpad=4)\n",
        "cax.yaxis.set_label_coords(2.5, 0.5)\n",
        "\n",
        "# Apply tight layout and save\n",
        "fig.tight_layout(pad=0.2)\n",
        "fig.savefig('Oliynyk_SHAP_beeswarm_Js_XGB.png', dpi=600, bbox_inches='tight', transparent=False)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Download plot in Colab environment\n",
        "try:\n",
        "    files.download('Oliynyk_SHAP_beeswarm_Js_XGB.png')\n",
        "except:\n",
        "    pass  # Skip download for local execution"
      ],
      "metadata": {
        "id": "PBWhZTVVBqe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================\n",
        "# CUSTOM COLORMAP FUNCTIONS\n",
        "# ====================================================================\n",
        "\n",
        "def shifted_coolwarm_cmap(p, n=256):\n",
        "    \"\"\"Shift coolwarm colormap so position p becomes neutral center.\"\"\"\n",
        "    base = plt.get_cmap(\"coolwarm\")\n",
        "    xs = np.linspace(0, 1, n)\n",
        "\n",
        "    cols = []\n",
        "    for x in xs:\n",
        "        if x <= p:\n",
        "            t = 0.5 * (x / p) if p > 0 else 0.0\n",
        "        else:\n",
        "            t = 0.5 + 0.5 * ((x - p) / (1 - p)) if p < 1 else 1.0\n",
        "        cols.append(base(t))\n",
        "\n",
        "    return mcolors.ListedColormap(cols, name=\"coolwarm_shifted\")\n",
        "\n",
        "\n",
        "def make_coolwarm_saturated_with_center(values, vcenter=0.0, nbins=6, steps=(1, 2, 2.5, 3, 5, 10)):\n",
        "    \"\"\"Create norm, cmap, and ticks for SHAP with saturated colors and zero at neutral.\"\"\"\n",
        "    x = np.asarray(values, dtype=float)\n",
        "    x = x[np.isfinite(x)]\n",
        "\n",
        "    # Fallback for empty data\n",
        "    if x.size == 0:\n",
        "        norm = mcolors.Normalize(vmin=-1, vmax=1, clip=True)\n",
        "        cmap = plt.get_cmap(\"coolwarm\")\n",
        "        ticks = np.array([-1, 0, 1], dtype=float)\n",
        "        return norm, cmap, ticks\n",
        "\n",
        "    data_min, data_max = float(np.min(x)), float(np.max(x))\n",
        "\n",
        "    # Normalize by actual data range for saturated colors\n",
        "    norm = mcolors.Normalize(vmin=data_min, vmax=data_max, clip=True)\n",
        "\n",
        "    # Generate ticks with scientific steps\n",
        "    loc = mticker.MaxNLocator(nbins=nbins, steps=list(steps))\n",
        "    ticks = loc.tick_values(data_min, data_max)\n",
        "    ticks = ticks[(ticks >= norm.vmin) & (ticks <= norm.vmax)]\n",
        "\n",
        "    # Shift colormap to place vcenter at neutral\n",
        "    if data_min < vcenter < data_max:\n",
        "        p = (vcenter - data_min) / (data_max - data_min)\n",
        "        cmap = shifted_coolwarm_cmap(p, n=256)\n",
        "\n",
        "        # Ensure vcenter is in ticks\n",
        "        if not np.any(np.isclose(ticks, vcenter)):\n",
        "            ticks = np.sort(np.append(ticks, vcenter))\n",
        "    else:\n",
        "        cmap = plt.get_cmap(\"coolwarm\")\n",
        "\n",
        "    return norm, cmap, ticks"
      ],
      "metadata": {
        "id": "589Uj5kcCw7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================\n",
        "# GRID-SHAP VISUALIZATION FOR XGBOOST\n",
        "# ====================================================================\n",
        "\n",
        "# Create DMatrix for XGBoost native SHAP calculation\n",
        "dmatrix_train = xgb.DMatrix(X_train, feature_names=X_train.columns.tolist())\n",
        "\n",
        "# Calculate SHAP values using XGBoost native API\n",
        "shap_values_raw = best_model.get_booster().predict(dmatrix_train, pred_contribs=True)\n",
        "shap_values_matrix = shap_values_raw[:, :-1]  # Feature contributions\n",
        "expected_value = float(shap_values_raw[0, -1])  # Base value\n",
        "\n",
        "# Create SHAP Explanation object\n",
        "explanation = shap.Explanation(\n",
        "    values=shap_values_matrix,\n",
        "    base_values=np.full(len(X_train), expected_value),\n",
        "    data=X_train.values,\n",
        "    feature_names=X_train.columns.tolist()\n",
        ")\n",
        "\n",
        "print(f\"Shape SHAP values: {shap_values_matrix.shape}\")\n",
        "print(f\"Expected value: {expected_value}\")\n",
        "\n",
        "# Identify top 5 most important features by mean absolute SHAP value\n",
        "mean_abs_shap_values = np.abs(explanation.values).mean(axis=0)\n",
        "top5_indices = np.argsort(mean_abs_shap_values)[-5:][::-1]\n",
        "\n",
        "# Create figure with 3x2 grid layout\n",
        "plt.style.use('default')\n",
        "fig = plt.figure(figsize=(8, 10))\n",
        "gs = GridSpec(3, 2, figure=fig, wspace=0.7, hspace=0.5)\n",
        "\n",
        "# Generate scatter plots for each top feature\n",
        "for i, feature_index in enumerate(top5_indices):\n",
        "    feature_name = X_train.columns[feature_index]\n",
        "    row = i // 2\n",
        "    col = i % 2\n",
        "\n",
        "    ax = fig.add_subplot(gs[row, col])\n",
        "    ax.set_facecolor('#f0f0f0')\n",
        "\n",
        "    # Extract SHAP values for current feature\n",
        "    shap_col = explanation[:, feature_index].values\n",
        "\n",
        "    # Get normalized colormap with zero at neutral center\n",
        "    norm, cmap_local, ticks = make_coolwarm_saturated_with_center(\n",
        "        shap_col,\n",
        "        vcenter=0.0,\n",
        "        nbins=6\n",
        "    )\n",
        "\n",
        "    # Create scatter plot: feature value vs target, colored by SHAP\n",
        "    sc = ax.scatter(\n",
        "        X_train[feature_name],\n",
        "        y_train,\n",
        "        c=shap_col,\n",
        "        cmap=cmap_local,\n",
        "        norm=norm,\n",
        "        alpha=0.9,\n",
        "        s=40,\n",
        "        edgecolor='black',\n",
        "        linewidth=0.25,\n",
        "        marker='o'\n",
        "    )\n",
        "\n",
        "    # Add colorbar with scientific notation\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "    cbar = fig.colorbar(sc, cax=cax)\n",
        "\n",
        "    # Configure colorbar ticks\n",
        "    cbar.set_ticks(ticks)\n",
        "    cbar.formatter = mticker.ScalarFormatter(useMathText=False)\n",
        "    cbar.formatter.set_powerlimits((-100, 100))\n",
        "    cbar.update_ticks()\n",
        "    cbar.ax.tick_params(labelsize=11)\n",
        "    cbar.set_label('SHAP Value', fontsize=12, labelpad=5)\n",
        "\n",
        "    # Configure axes\n",
        "    ax.yaxis.set_major_locator(mticker.MaxNLocator(min_n_ticks=4, nbins=5))\n",
        "    ax.xaxis.set_major_locator(mticker.MaxNLocator(min_n_ticks=3, nbins=5, integer=True))\n",
        "    ax.set_xlabel(feature_name, fontsize=12, labelpad=4)\n",
        "    ax.set_ylabel(\"Saturation Polarization (T)\", fontsize=12, labelpad=4)\n",
        "    ax.tick_params(axis='both', labelsize=11, pad=4)\n",
        "    ax.grid(True, linestyle=':', alpha=0.35, linewidth=0.45)\n",
        "\n",
        "# Save high-resolution figure\n",
        "plt.savefig('Oliynyk_Grid-SHAP_Js_XGB.png',\n",
        "            dpi=600,\n",
        "            bbox_inches='tight',\n",
        "            facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "# Download in Colab environment\n",
        "try:\n",
        "    files.download('Oliynyk_Grid-SHAP_Js_XGB.png')\n",
        "except:\n",
        "    pass  # Skip download for local execution"
      ],
      "metadata": {
        "id": "uVtDwyQyC2ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L8Hqqh5wFLb"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_OqZ3uOwLks"
      },
      "outputs": [],
      "source": [
        "# After evaluating all models, add this to display and save results:\n",
        "def display_model_results(results_df):\n",
        "    \"\"\"Display and save model comparison results\"\"\"\n",
        "    # Format display\n",
        "    pd.options.display.float_format = '{:.4f}'.format\n",
        "    results_df.set_index('Model', inplace=True)\n",
        "\n",
        "    print(\"\\nFinal Model Comparison:\")\n",
        "    print(\"=\"*60)\n",
        "    print(results_df)\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Highlight best scores\n",
        "    def highlight_metrics(s):\n",
        "        metrics = {'MAE': 'min', 'MSE': 'min', 'RMSE': 'min', 'R²': 'max'}\n",
        "        styles = []\n",
        "        for v in s:\n",
        "            if s.name in metrics:\n",
        "                if metrics[s.name] == 'min' and v == s.min():\n",
        "                    styles.append('background-color: yellow')\n",
        "                elif metrics[s.name] == 'max' and v == s.max():\n",
        "                    styles.append('background-color: lightgreen')\n",
        "                else:\n",
        "                    styles.append('')\n",
        "            else:\n",
        "                styles.append('')\n",
        "        return styles\n",
        "\n",
        "    styled_df = results_df.style.apply(highlight_metrics)\n",
        "    display(styled_df)\n",
        "\n",
        "    # Save to Excel\n",
        "    results_df.to_excel('Oliynyk_Js_metrics.xlsx')\n",
        "    print(\"\\nResults saved to 'Oliynyk_Js_metrics.xlsx'\")\n",
        "\n",
        "\n",
        "# Call this after evaluating all models\n",
        "display_model_results(model_results)\n",
        "\n",
        "# Download file (Colab only)\n",
        "try:\n",
        "    files.download('Oliynyk_Js_metrics.xlsx')\n",
        "except:\n",
        "    pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}