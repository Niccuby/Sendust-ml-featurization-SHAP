{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Y-NWG5FKXM9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib.ticker as mticker\n",
        "import matplotlib.colors as mcolors\n",
        "import shap\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.utils import shuffle\n",
        "import joblib\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # For Colab\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    df = pd.read_csv('Composition_dataset_wt%.csv')\n",
        "except:\n",
        "    # For local execution\n",
        "    df = pd.read_csv('../data/training/Composition_dataset_wt%.csv')\n",
        "\n",
        "print(f\"Loaded {len(df)} samples with {len(df.columns)} features\")"
      ],
      "metadata": {
        "id": "f1cUQ6feKqyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# CONFIGURATION\n",
        "# ========================================\n",
        "RUN_OPTIMIZATION = False  # True: run hyperparameter tuning | False: use pre-optimized parameters\n",
        "USE_EXTERNAL_VALIDATION = True  # True: run external validation | False: skip validation\n",
        "SAVE_MODEL = False  # True: save model to .pkl file | False: skip saving\n",
        "\n",
        "# Optuna optimization settings (use None to skip parameter)\n",
        "# Example:\n",
        "OPTUNA_CONFIG = {\n",
        "    'RFR': {'n_trials': 100, 'timeout': None},   # Only 100 trials\n",
        "    'ETR': {'n_trials': None, 'timeout': 600},   # Only 10 min timeout\n",
        "    'GBR': {'n_trials': 150, 'timeout': 900},    # Both: 150 trials or 15 min\n",
        "    'XGB': {'n_trials': 150, 'timeout': 900}     # Both: 150 trials or 15 min\n",
        "}"
      ],
      "metadata": {
        "id": "V3crDCqnM-eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if RUN_OPTIMIZATION:\n",
        "    # For Google Colab: uncomment the line below to install Optuna\n",
        "    !pip install -q optuna\n",
        "\n",
        "    # For local use: install via requirements.txt\n",
        "    # pip install -r requirements.txt\n",
        "\n",
        "    import optuna\n",
        "    import  optuna.visualization as vis"
      ],
      "metadata": {
        "id": "nAxoyumniu4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_EXTERNAL_VALIDATION:\n",
        "    # Configuration: Set target property and feature type\n",
        "    property_name = \"rho\"  # \"Hc\", \"Js\", or \"rho\"\n",
        "    featurization_method = \"Composition\"  # \"Composition\", \"WenAlloys\", or \"CBFV\"\n",
        "    output_file = f\"predictions_{property_name}_{featurization_method}.xlsx\"\n",
        "    column_name = f\"{property_name}_predict\"\n",
        "\n",
        "    # Load experimental dataset\n",
        "    try:\n",
        "        # Colab: upload file\n",
        "        uploaded = files.upload()\n",
        "        X_predict = pd.read_csv('Dataset_experimental_wt%.csv')\n",
        "    except:\n",
        "        # Local: load from directory\n",
        "        X_predict = pd.read_csv('../data/external_validation/Dataset_experimental_wt%.csv')\n",
        "\n",
        "    print(f\"Loaded {len(X_predict)} experimental samples with {len(X_predict.columns)} features\")\n",
        "    display(X_predict.head())\n",
        "\n",
        "    # Initialize prediction results table (5 alloys × 2 models)\n",
        "    results = pd.DataFrame({\n",
        "        \"Model\": [\"ETR\"]*5 + [\"XGB\"]*5,\n",
        "        \"Alloy_number\": list(range(1, 6)) + list(range(1, 6)),\n",
        "        column_name: [np.nan]*10\n",
        "    })\n",
        "\n",
        "    print(f\"\\nPrediction table initialized for {property_name} ({featurization_method})\")\n",
        "    print(results, \"\\n\")"
      ],
      "metadata": {
        "id": "DpgMzhP-izdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploratory Data Analysis\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "vYnFKc3Vi3Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "df_drop = df.dropna(subset=['Resistivity'])  # Remove rows with missing target values\n",
        "df_drop = df_drop.reset_index(drop=True)     # Reset index after dropping"
      ],
      "metadata": {
        "id": "CYNg7Eq6i4ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature and Target Extraction\n",
        "features = df_drop.iloc[:, 0:3]              # Si, Al, Fe composition (wt.%)\n",
        "target= df_drop.loc[:, 'Resistivity']      # Target: Resistivity (μΩ⋅cm)"
      ],
      "metadata": {
        "id": "_gULUOmTjDaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Shuffling and Splitting\n",
        "features, target = shuffle(features, target, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features, target, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}, Testing samples: {len(X_test)}\")"
      ],
      "metadata": {
        "id": "phfvORTbWfBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optuna Hyperparameters Optimization"
      ],
      "metadata": {
        "id": "Kgay2WkPp5zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty DataFrame to store all results\n",
        "model_results = pd.DataFrame(columns=['Model', 'MAE', 'MSE', 'RMSE', 'R²'])"
      ],
      "metadata": {
        "id": "p-jYzRbhvgVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "JWmvN1JPrBiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    \"\"\"Optuna objective for Random Forest hyperparameter tuning.\"\"\"\n",
        "\n",
        "    # Hyperparameter search space\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 300, 500),\n",
        "        'max_depth': trial.suggest_int('max_depth', 9, 30),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 5),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 2),\n",
        "        'criterion': trial.suggest_categorical('criterion', ['squared_error', 'absolute_error', 'friedman_mse', 'poisson']),\n",
        "    }\n",
        "\n",
        "    # Evaluate with 5-fold CV\n",
        "    model = RandomForestRegressor(random_state=42, **params)\n",
        "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='r2').mean()\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "w8l5di0a5X3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if RUN_OPTIMIZATION:\n",
        "    print(\"Running hyperparameter optimization...\")\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "\n",
        "    # Extract config and filter out None values\n",
        "    config = OPTUNA_CONFIG['RFR']\n",
        "    optimize_kwargs = {k: v for k, v in config.items() if v is not None}\n",
        "\n",
        "    study.optimize(objective, **optimize_kwargs)\n",
        "\n",
        "    # Display results\n",
        "    print('\\nBest trial results:')\n",
        "    trial = study.best_trial\n",
        "    print(f'R2 score: {trial.value:.4f}\\n')\n",
        "\n",
        "    print('Best parameters:')\n",
        "    print('best_params = {')\n",
        "    for i, (key, value) in enumerate(trial.params.items()):\n",
        "        if isinstance(value, float):\n",
        "            print(f\"    '{key}': {value},\")\n",
        "        elif isinstance(value, str):\n",
        "            print(f\"    '{key}': '{value}',\")\n",
        "        else:\n",
        "            print(f\"    '{key}': {value},\")\n",
        "    print('}\\n')\n",
        "\n",
        "    best_params = study.best_params\n",
        "\n",
        "    # Display Optuna optimization visualizations\n",
        "    vis.plot_optimization_history(study).show()\n",
        "    vis.plot_param_importances(study).show()\n",
        "    vis.plot_parallel_coordinate(study).show()\n",
        "\n",
        "else:\n",
        "    # Use pre-optimized parameters\n",
        "    print(\"Using pre-optimized hyperparameters...\")\n",
        "    best_params = {\n",
        "        'n_estimators': 460,\n",
        "        'max_depth': 10,\n",
        "        'min_samples_split': 2,\n",
        "        'min_samples_leaf': 1,\n",
        "        'criterion': 'absolute_error'\n",
        "    }\n",
        "    print(f\"Parameters: {best_params}\\n\")\n",
        "\n",
        "# Train model with best parameters\n",
        "best_model = RandomForestRegressor(random_state=42, **best_params)\n",
        "best_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "62m-b_09qcLA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if SAVE_MODEL:\n",
        "    # Save trained model\n",
        "    joblib.dump(best_model, '1_composition_Resistivity_rfr.pkl')\n",
        "    print(\"✅ Model saved as '1_composition_Resistivity_rfr.pkl'\")\n",
        "\n",
        "    try:\n",
        "        files.download('1_composition_Resistivity_rfr.pkl')\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "L6gPqp-Bmera"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "test_r2 = r2_score(y_test, y_pred)\n",
        "test_mae = mean_absolute_error(y_test, y_pred)\n",
        "test_mse = mean_squared_error(y_test, y_pred)\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "\n",
        "# Display metrics in formatted table\n",
        "print(\"\\nTest Set Evaluation Metrics:\")\n",
        "print(\"----------------------------------------\")\n",
        "print(f\"| {'Metric':<10} | {'Value':>15} |\")\n",
        "print(\"|------------|----------------|\")\n",
        "print(f\"| R²         | {test_r2:>15.4f} |\")\n",
        "print(f\"| MAE        | {test_mae:>15.4f} |\")\n",
        "print(f\"| MSE        | {test_mse:>15.4f} |\")\n",
        "print(f\"| RMSE       | {test_rmse:>15.4f} |\")"
      ],
      "metadata": {
        "id": "uyYfC-FbqoS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_and_store(model, model_name, X_test, y_test, results_df):\n",
        "    \"\"\"Evaluate model and store metrics in results DataFrame.\"\"\"\n",
        "\n",
        "    # Predict and calculate metrics\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'MAE': mean_absolute_error(y_test, y_pred),\n",
        "        'MSE': mean_squared_error(y_test, y_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "        'R²': r2_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    # Add to results\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([metrics])], ignore_index=True)\n",
        "    return results_df\n",
        "\n",
        "\n",
        "# Store RFR evaluation results\n",
        "model_results = evaluate_and_store(best_model, 'RFR', X_test, y_test, model_results)"
      ],
      "metadata": {
        "id": "HaIP9xkAv3gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra Trees Regressor"
      ],
      "metadata": {
        "id": "hVpvv6W7mf03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    \"\"\"Optuna objective for Extra Trees hyperparameter tuning.\"\"\"\n",
        "\n",
        "    # Hyperparameter search space\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
        "        'max_depth': trial.suggest_int('max_depth', 8, 30),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 8),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 2),\n",
        "        'criterion': trial.suggest_categorical('criterion', ['squared_error', 'absolute_error', 'friedman_mse', 'poisson']),\n",
        "            }\n",
        "\n",
        "    # Evaluate with 5-fold CV\n",
        "    model = ExtraTreesRegressor(random_state=42, **params)\n",
        "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='r2').mean()\n",
        "    return score"
      ],
      "metadata": {
        "id": "ciZIx3FH8f2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if RUN_OPTIMIZATION:\n",
        "    print(\"Running hyperparameter optimization...\")\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "\n",
        "    # Extract config and filter out None values\n",
        "    config = OPTUNA_CONFIG['ETR']\n",
        "    optimize_kwargs = {k: v for k, v in config.items() if v is not None}\n",
        "\n",
        "    study.optimize(objective, **optimize_kwargs)\n",
        "\n",
        "    # Display results\n",
        "    print('\\nBest trial results:')\n",
        "    trial = study.best_trial\n",
        "    print(f'R2 score: {trial.value:.4f}\\n')\n",
        "\n",
        "    print('Best parameters:')\n",
        "    print('best_params = {')\n",
        "    for i, (key, value) in enumerate(trial.params.items()):\n",
        "        if isinstance(value, float):\n",
        "            print(f\"    '{key}': {value},\")\n",
        "        elif isinstance(value, str):\n",
        "            print(f\"    '{key}': '{value}',\")\n",
        "        else:\n",
        "            print(f\"    '{key}': {value},\")\n",
        "    print('}\\n')\n",
        "\n",
        "    best_params = study.best_params\n",
        "\n",
        "    # Display Optuna optimization visualizations\n",
        "    vis.plot_optimization_history(study).show()\n",
        "    vis.plot_param_importances(study).show()\n",
        "    vis.plot_parallel_coordinate(study).show()\n",
        "\n",
        "else:\n",
        "    # Use pre-optimized parameters\n",
        "    print(\"Using pre-optimized hyperparameters...\")\n",
        "    best_params = {\n",
        "        'n_estimators': 304,\n",
        "        'max_depth': 8,\n",
        "        'min_samples_split': 2,\n",
        "        'min_samples_leaf': 1,\n",
        "        'criterion': 'absolute_error'\n",
        "    }\n",
        "    print(f\"Parameters: {best_params}\\n\")\n",
        "\n",
        "# Train model with best parameters\n",
        "best_model = ExtraTreesRegressor(random_state=42, **best_params)\n",
        "best_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "QCBvfuMgplhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if SAVE_MODEL:\n",
        "\n",
        "# Save model to .pkl file\n",
        "    joblib.dump(best_model, '2_composition_Resistivity_etr.pkl')\n",
        "\n",
        "    print(\"✅ Model saved as '2_composition_Resistivity_etr.pkl'\")\n",
        "    try:\n",
        "        files.download('2_composition_Resistivity_etr.pkl')\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "TW9Mo_XKluoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "test_r2 = r2_score(y_test, y_pred)\n",
        "test_mae = mean_absolute_error(y_test, y_pred)\n",
        "test_mse = mean_squared_error(y_test, y_pred)\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "\n",
        "# Display metrics in formatted table\n",
        "print(\"\\nTest Set Evaluation Metrics:\")\n",
        "print(\"----------------------------------------\")\n",
        "print(f\"| {'Metric':<10} | {'Value':>15} |\")\n",
        "print(\"|------------|----------------|\")\n",
        "print(f\"| R²         | {test_r2:>15.4f} |\")\n",
        "print(f\"| MAE        | {test_mae:>15.4f} |\")\n",
        "print(f\"| MSE        | {test_mse:>15.4f} |\")\n",
        "print(f\"| RMSE       | {test_rmse:>15.4f} |\")\n",
        "print(\"----------------------------------------\")"
      ],
      "metadata": {
        "id": "1MfF7yTBn7f2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_EXTERNAL_VALIDATION:\n",
        "    # Make predictions on external validation set (ETR)\n",
        "    y_pred_etr = best_model.predict(X_predict)\n",
        "\n",
        "    # Save ETR predictions to results DataFrame\n",
        "    results.loc[results[\"Model\"] == \"ETR\", column_name] = y_pred_etr\n",
        "    print(f\"ETR predictions saved: {y_pred_etr}\")\n",
        "    print(results)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "L6TU1zJzq8Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_and_store(model, model_name, X_test, y_test, results_df):\n",
        "    \"\"\"Evaluate model and store metrics in results DataFrame.\"\"\"\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate regression metrics\n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'MAE': mean_absolute_error(y_test, y_pred),\n",
        "        'MSE': mean_squared_error(y_test, y_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "        'R²': r2_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    # Append metrics to results DataFrame\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([metrics])], ignore_index=True)\n",
        "    return results_df\n",
        "\n",
        "\n",
        "# Evaluate ETR model and store results\n",
        "model_results = evaluate_and_store(best_model, 'ETR', X_test, y_test, model_results)"
      ],
      "metadata": {
        "id": "YilHJfefw03S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shifted_coolwarm_cmap(p, n=256):\n",
        "    \"\"\"Shift coolwarm colormap so position p becomes neutral center.\"\"\"\n",
        "    base = plt.get_cmap(\"coolwarm\")\n",
        "    xs = np.linspace(0, 1, n)\n",
        "\n",
        "    cols = []\n",
        "    for x in xs:\n",
        "        if x <= p:\n",
        "            t = 0.5 * (x / p) if p > 0 else 0.0\n",
        "        else:\n",
        "            t = 0.5 + 0.5 * ((x - p) / (1 - p)) if p < 1 else 1.0\n",
        "        cols.append(base(t))\n",
        "\n",
        "    return mcolors.ListedColormap(cols, name=\"coolwarm_shifted\")\n",
        "\n",
        "\n",
        "\n",
        "def make_coolwarm_saturated_with_center(values, vcenter=0.0, nbins=6, steps=(1, 2, 2.5, 5, 10)):\n",
        "    \"\"\"Create norm, cmap, and ticks for SHAP with saturated colors and zero at neutral.\"\"\"\n",
        "    x = np.asarray(values, dtype=float)\n",
        "    x = x[np.isfinite(x)]\n",
        "\n",
        "    # Fallback for empty data\n",
        "    if x.size == 0:\n",
        "        norm = mcolors.Normalize(vmin=-1, vmax=1, clip=True)\n",
        "        cmap = plt.get_cmap(\"coolwarm\")\n",
        "        ticks = np.array([-1, 0, 1], dtype=float)\n",
        "        return norm, cmap, ticks\n",
        "\n",
        "    data_min, data_max = float(np.min(x)), float(np.max(x))\n",
        "\n",
        "    # Normalize by actual data range for saturated colors\n",
        "    norm = mcolors.Normalize(vmin=data_min, vmax=data_max, clip=True)\n",
        "\n",
        "    # Generate scientific ticks\n",
        "    loc = mticker.MaxNLocator(nbins=nbins, steps=list(steps))\n",
        "    ticks = loc.tick_values(data_min, data_max)\n",
        "    ticks = ticks[(ticks >= norm.vmin) & (ticks <= norm.vmax)]\n",
        "\n",
        "    # Shift colormap to place vcenter at neutral\n",
        "    if data_min < vcenter < data_max:\n",
        "        p = (vcenter - data_min) / (data_max - data_min)\n",
        "        cmap = shifted_coolwarm_cmap(p, n=256)\n",
        "\n",
        "        # Ensure vcenter is in ticks\n",
        "        if not np.any(np.isclose(ticks, vcenter)):\n",
        "            ticks = np.sort(np.append(ticks, vcenter))\n",
        "    else:\n",
        "        cmap = plt.get_cmap(\"coolwarm\")\n",
        "\n",
        "    return norm, cmap, ticks"
      ],
      "metadata": {
        "id": "1ePvFZ3nLil-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ CALCULATE SHAP VALUES ============\n",
        "explainer = shap.TreeExplainer(best_model, X_train)\n",
        "shap_values = explainer(X_train)\n",
        "\n",
        "# Create Explanation object (optional)\n",
        "explanation = shap.Explanation(\n",
        "    values=shap_values.values,\n",
        "    base_values=explainer.expected_value,\n",
        "    data=X_train.values,\n",
        "    feature_names=X_train.columns.tolist()\n",
        ")\n",
        "\n",
        "# ============ DEFINE FEATURE ORDER ============\n",
        "desired_order = ['Fe', 'Si', 'Al']\n",
        "\n",
        "# ============ BUILD GRID-SHAP PLOTS ============\n",
        "plt.style.use('default')\n",
        "fig = plt.figure(figsize=(3.5, 10))\n",
        "gs = GridSpec(3, 1, figure=fig, hspace=0.3)\n",
        "\n",
        "for i, feature_name in enumerate(desired_order):\n",
        "    # Get feature index\n",
        "    try:\n",
        "        feature_index = X_train.columns.get_loc(feature_name)\n",
        "    except KeyError:\n",
        "        print(f\"Error: Feature '{feature_name}' not found in X_train.\")\n",
        "        continue\n",
        "\n",
        "    ax = fig.add_subplot(gs[i, 0])\n",
        "    ax.set_facecolor('#f0f0f0')\n",
        "\n",
        "    # Extract SHAP values for current feature\n",
        "    shap_col = shap_values[:, feature_index].values\n",
        "\n",
        "    # Get normalized colormap with zero at neutral\n",
        "    norm, cmap_local, ticks = make_coolwarm_saturated_with_center(\n",
        "        shap_col,\n",
        "        vcenter=0.0,\n",
        "        nbins=6\n",
        "    )\n",
        "\n",
        "    # Scatter plot\n",
        "    sc = ax.scatter(\n",
        "        X_train[feature_name],\n",
        "        y_train,\n",
        "        c=shap_col,\n",
        "        cmap=cmap_local,\n",
        "        norm=norm,\n",
        "        alpha=0.9,\n",
        "        s=50,\n",
        "        edgecolor='black',\n",
        "        linewidth=0.3,\n",
        "        marker='o'\n",
        "    )\n",
        "\n",
        "    # Add colorbar\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes('right', size='4.5%', pad=0.05)\n",
        "    cbar = fig.colorbar(sc, cax=cax)\n",
        "\n",
        "    # Set scientific ticks\n",
        "    cbar.set_ticks(ticks)\n",
        "    cbar.formatter = mticker.ScalarFormatter(useMathText=False)\n",
        "    cbar.formatter.set_powerlimits((-100, 100))\n",
        "    cbar.update_ticks()\n",
        "    cbar.ax.tick_params(labelsize=11)\n",
        "    cbar.set_label('SHAP Value', fontsize=12, labelpad=5)\n",
        "\n",
        "    # Configure axes\n",
        "    ax.yaxis.set_major_locator(mticker.MaxNLocator(nbins=5, integer=True))\n",
        "    ax.xaxis.set_major_locator(mticker.MaxNLocator(min_n_ticks=3, nbins=4, integer=True))\n",
        "    ax.set_xlabel(feature_name, fontsize=12, labelpad=4)\n",
        "    ax.set_ylabel(\"Resistivity (μΩ⋅cm)\", fontsize=12, labelpad=4)\n",
        "    ax.tick_params(axis='both', labelsize=11, pad=4)\n",
        "    ax.grid(True, linestyle=':', alpha=0.35, linewidth=0.45)\n",
        "\n",
        "# Save figure\n",
        "plt.savefig('Composition_Grid-SHAP_rho_ETR.png',\n",
        "            dpi=600,\n",
        "            bbox_inches='tight',\n",
        "            facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "# Download in Colab (auto-skip locally)\n",
        "try:\n",
        "    files.download('Composition_Grid-SHAP_rho_ETR.png')\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "1ceWHblUC2PI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Boost Regressor"
      ],
      "metadata": {
        "id": "9KYoSztSflco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    \"\"\"Optuna objective for Gradient Boost hyperparameter tuning.\"\"\"\n",
        "\n",
        "    # Hyperparameter search space\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 420, 500),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 8, 18),\n",
        "        'min_samples_leaf': 2,\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 0.6),\n",
        "        'loss': 'absolute_error',\n",
        "        'criterion': 'friedman_mse',\n",
        "    }\n",
        "\n",
        "    # Evaluate with 5-fold CV\n",
        "    model = GradientBoostingRegressor(random_state=42, **params)\n",
        "    score = cross_val_score(model, X_train, y_train,\n",
        "                          cv=5, scoring='r2').mean()\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "IKH7jmkWAz7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if RUN_OPTIMIZATION:\n",
        "    print(\"Running hyperparameter optimization...\")\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "\n",
        "    # Extract config and filter out None values\n",
        "    config = OPTUNA_CONFIG['GBR']\n",
        "    optimize_kwargs = {k: v for k, v in config.items() if v is not None}\n",
        "\n",
        "    study.optimize(objective, **optimize_kwargs)\n",
        "\n",
        "    # Display results\n",
        "    print('\\nBest trial results:')\n",
        "    trial = study.best_trial\n",
        "    print(f'R2 score: {trial.value:.4f}\\n')\n",
        "\n",
        "    print('Best parameters:')\n",
        "    print('best_params = {')\n",
        "    for i, (key, value) in enumerate(trial.params.items()):\n",
        "        if isinstance(value, float):\n",
        "            print(f\"    '{key}': {value},\")\n",
        "        elif isinstance(value, str):\n",
        "            print(f\"    '{key}': '{value}',\")\n",
        "        else:\n",
        "            print(f\"    '{key}': {value},\")\n",
        "    print('}\\n')\n",
        "\n",
        "    best_params = study.best_params\n",
        "\n",
        "    # Display Optuna optimization visualizations\n",
        "    vis.plot_optimization_history(study).show()\n",
        "    vis.plot_param_importances(study).show()\n",
        "    vis.plot_parallel_coordinate(study).show()\n",
        "\n",
        "else:\n",
        "    # Use pre-optimized parameters\n",
        "    print(\"Using pre-optimized hyperparameters...\")\n",
        "    best_params = {\n",
        "        'n_estimators': 442,\n",
        "        'learning_rate': 0.04,\n",
        "        'max_depth': 3,\n",
        "        'min_samples_split': 17,\n",
        "        'subsample': 0.8\n",
        "    }\n",
        "    print(f\"Parameters: {best_params}\\n\")\n",
        "\n",
        "\n",
        "# Train model with best parameters\n",
        "best_model = GradientBoostingRegressor(random_state=42, **best_params)\n",
        "best_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "pNDvJeeJunuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if SAVE_MODEL:\n",
        "\n",
        "# Save model to .pkl file\n",
        "    joblib.dump(best_model, '3_composition_Resistivity_gbr.pkl')\n",
        "\n",
        "    print(\"✅ Model saved as '3_composition_Resistivity_gbr.pkl'\")\n",
        "    try:\n",
        "        files.download('3_composition_Resistivity_gbr.pkl')\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "bETiVZ5cyWI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "test_r2 = r2_score(y_test, y_pred)\n",
        "test_mae = mean_absolute_error(y_test, y_pred)\n",
        "test_mse = mean_squared_error(y_test, y_pred)\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "\n",
        "# Display metrics in formatted table\n",
        "print(\"\\nTest Set Evaluation Metrics:\")\n",
        "print(\"----------------------------------------\")\n",
        "print(f\"| {'Metric':<10} | {'Value':>15} |\")\n",
        "print(\"|------------|----------------|\")\n",
        "print(f\"| R²         | {test_r2:>15.4f} |\")\n",
        "print(f\"| MAE        | {test_mae:>15.4f} |\")\n",
        "print(f\"| MSE        | {test_mse:>15.4f} |\")\n",
        "print(f\"| RMSE       | {test_rmse:>15.4f} |\")\n",
        "print(\"----------------------------------------\")"
      ],
      "metadata": {
        "id": "IeqtdUwyEWr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_and_store(model, model_name, X_test, y_test, results_df):\n",
        "    \"\"\"Evaluate model and store metrics in results DataFrame.\"\"\"\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate regression metrics\n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'MAE': mean_absolute_error(y_test, y_pred),\n",
        "        'MSE': mean_squared_error(y_test, y_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "        'R²': r2_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    # Append metrics to results DataFrame\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([metrics])], ignore_index=True)\n",
        "    return results_df\n",
        "\n",
        "\n",
        "# Evaluate GBR model and store results\n",
        "model_results = evaluate_and_store(best_model, 'GBR', X_test, y_test, model_results)"
      ],
      "metadata": {
        "id": "sgTYWDUsxBLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost Regressor"
      ],
      "metadata": {
        "id": "wS9JUMRRr8g-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    params = {\n",
        "        'objective': 'reg:squarederror',\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 400, 500),\n",
        "        'max_depth': trial.suggest_int('max_depth', 6, 10),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.2, 0.3, log=True),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 0.7),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.9, 1.0),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 2),\n",
        "        'gamma': trial.suggest_float('gamma', 0, 0.3),\n",
        "        'alpha': trial.suggest_float('alpha', 4, 6),\n",
        "        'lambda': trial.suggest_float('lambda', 8.5, 10),\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBRegressor(random_state=42, **params)\n",
        "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='r2').mean()\n",
        "    return score"
      ],
      "metadata": {
        "id": "l4az-7zksM4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    \"\"\"Optuna objective for Gradient Boost hyperparameter tuning.\"\"\"\n",
        "\n",
        "    # Hyperparameter search space\n",
        "    params = {\n",
        "        'objective': 'reg:squarederror',\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 400, 500),\n",
        "        'max_depth': trial.suggest_int('max_depth', 6, 10),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.2, 0.3, log=True),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 0.7),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.9, 1.0),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 2),\n",
        "        'gamma': trial.suggest_float('gamma', 0, 0.3),\n",
        "        'alpha': trial.suggest_float('alpha', 4, 6),\n",
        "        'lambda': trial.suggest_float('lambda', 8.5, 10),\n",
        "    }\n",
        "\n",
        "    # Evaluate with 5-fold CV\n",
        "    model = xgb.XGBRegressor(random_state=42, **params)\n",
        "    score = cross_val_score(model, X_train, y_train,\n",
        "                            cv=5, scoring='r2').mean()\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "CrtxYDbpBoNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if RUN_OPTIMIZATION:\n",
        "    print(\"Running hyperparameter optimization...\")\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "\n",
        "    # Extract config and filter out None values\n",
        "    config = OPTUNA_CONFIG['XGB']\n",
        "    optimize_kwargs = {k: v for k, v in config.items() if v is not None}\n",
        "\n",
        "    study.optimize(objective, **optimize_kwargs)\n",
        "\n",
        "    # Display results\n",
        "    print('\\nBest trial results:')\n",
        "    trial = study.best_trial\n",
        "    print(f'R2 score: {trial.value:.4f}\\n')\n",
        "\n",
        "    print('Best parameters:')\n",
        "    print('best_params = {')\n",
        "    for i, (key, value) in enumerate(trial.params.items()):\n",
        "        if isinstance(value, float):\n",
        "            print(f\"    '{key}': {value},\")\n",
        "        elif isinstance(value, str):\n",
        "            print(f\"    '{key}': '{value}',\")\n",
        "        else:\n",
        "            print(f\"    '{key}': {value},\")\n",
        "    print('}\\n')\n",
        "\n",
        "    best_params = study.best_params\n",
        "\n",
        "    # Display Optuna optimization visualizations\n",
        "    vis.plot_optimization_history(study).show()\n",
        "    vis.plot_param_importances(study).show()\n",
        "    vis.plot_parallel_coordinate(study).show()\n",
        "\n",
        "else:\n",
        "    # Use pre-optimized parameters\n",
        "    print(\"Using pre-optimized hyperparameters...\")\n",
        "    best_params = {\n",
        "        'objective': 'reg:squarederror',\n",
        "        'n_estimators': 461,\n",
        "        'max_depth': 6,\n",
        "        'learning_rate': 0.2345269441539146,\n",
        "        'subsample': 0.6524034514469835,\n",
        "        'colsample_bytree': 0.9817378593570328,\n",
        "        'min_child_weight': 2,\n",
        "        'gamma': 0.2998781940594144,\n",
        "        'alpha': 5.422366535485192,\n",
        "        'reg_lambda': 9.3158705998517\n",
        "    }\n",
        "    print(f\"Parameters: {best_params}\\n\")\n",
        "\n",
        "\n",
        "# Train model with best parameters\n",
        "mean_target = float(y_train.mean())\n",
        "best_model = xgb.XGBRegressor(random_state=42, **best_params)\n",
        "best_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "dN7uOsdXv6Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if SAVE_MODEL:\n",
        "\n",
        "# Save model to .pkl file\n",
        "    joblib.dump(best_model, '4_composition_Resistivity_xgb.pkl')\n",
        "\n",
        "    print(\"✅ Model saved as '4_composition_Resistivity_xgb'\")\n",
        "    try:\n",
        "        files.download('4_composition_Resistivity_xgb.pkl')\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "KTVASnauyjVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "test_r2 = r2_score(y_test, y_pred)\n",
        "test_mae = mean_absolute_error(y_test, y_pred)\n",
        "test_mse = mean_squared_error(y_test, y_pred)\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "\n",
        "# Display metrics in formatted table\n",
        "print(\"\\nTest Set Evaluation Metrics:\")\n",
        "print(\"----------------------------------------\")\n",
        "print(f\"| {'Metric':<10} | {'Value':>15} |\")\n",
        "print(\"|------------|----------------|\")\n",
        "print(f\"| R²         | {test_r2:>15.4f} |\")\n",
        "print(f\"| MAE        | {test_mae:>15.4f} |\")\n",
        "print(f\"| MSE        | {test_mse:>15.4f} |\")\n",
        "print(f\"| RMSE       | {test_rmse:>15.4f} |\")\n",
        "print(\"----------------------------------------\")"
      ],
      "metadata": {
        "id": "PsQH4ijDuQAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_EXTERNAL_VALIDATION:\n",
        "    # Make predictions on external validation set (XGB)\n",
        "    y_pred_xgb = best_model.predict(X_predict)\n",
        "\n",
        "    # Save XGB predictions to results DataFrame\n",
        "    results.loc[results[\"Model\"] == \"XGB\", column_name] = y_pred_xgb\n",
        "    print(f\"XGB predictions saved: {y_pred_xgb}\")\n",
        "    print(results)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Add experimental ground truth values\n",
        "    y_true = [86.8, 106.9, 146.1, 122.8, 101.2]  # ← UPDATE WITH ACTUAL VALUES\n",
        "    results[\"y_true\"] = y_true * 2  # Duplicate for both models\n",
        "\n",
        "    # Calculate errors\n",
        "    results[\"error\"] = results[column_name] - results[\"y_true\"]\n",
        "    results[\"abs_error\"] = results[\"error\"].abs()\n",
        "    results[\"abs_percent_error_%\"] = 100 * results[\"abs_error\"] / results[\"y_true\"]\n",
        "\n",
        "    # Calculate average metrics per model\n",
        "    for model in [\"ETR\", \"XGB\"]:\n",
        "        mask = results[\"Model\"] == model\n",
        "        mae = results.loc[mask, \"abs_error\"].mean()\n",
        "        mape = results.loc[mask, \"abs_percent_error_%\"].mean()\n",
        "        print(f\"{model}: MAE = {mae:.4f}, MAPE = {mape:.2f}%\")\n",
        "\n",
        "    # Save results to Excel\n",
        "    results.to_excel(output_file, index=False, engine='openpyxl')\n",
        "    print(f\"✓ Results saved to {output_file}\")\n",
        "\n",
        "    # Download file (Colab only)\n",
        "    try:\n",
        "        files.download(output_file)\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "i_-WiKaSzLPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_and_store(model, model_name, X_test, y_test, results_df):\n",
        "    \"\"\"\n",
        "    Evaluate model and store results in the results DataFrame\n",
        "    Returns updated DataFrame\n",
        "    \"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'MAE': mean_absolute_error(y_test, y_pred),\n",
        "        'MSE': mean_squared_error(y_test, y_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "        'R²': r2_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([metrics])], ignore_index=True)\n",
        "    return results_df\n",
        "\n",
        "\n",
        "model_results = evaluate_and_store(best_model, 'XGB', X_test, y_test, model_results)"
      ],
      "metadata": {
        "id": "WVZfYUlwxJT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_train_test_relationship(model, X_train, y_train, X_test, y_test, model_name=\"Model\"):\n",
        "    \"\"\"Plot actual vs predicted values for train and test sets.\"\"\"\n",
        "\n",
        "    # Initialize figure with publication quality settings\n",
        "    plt.figure(figsize=(8, 6), dpi=300)\n",
        "\n",
        "    # Generate predictions for both datasets\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # Plot training and test data with distinct colors\n",
        "    plt.scatter(y_train, y_train_pred, alpha=0.8, label='Training', color='#2408b1', s=150)\n",
        "    plt.scatter(y_test, y_test_pred, alpha=0.8, label='Test', color='#ca4016', s=150)\n",
        "\n",
        "    # Add ideal prediction line (y = x)\n",
        "    max_val = max(np.max(y_train), np.max(y_test), np.max(y_train_pred), np.max(y_test_pred))\n",
        "    min_val = min(np.min(y_train), np.min(y_test), np.min(y_train_pred), np.min(y_test_pred))\n",
        "    plt.plot([min_val, max_val], [min_val, max_val], '--', color='black',\n",
        "             label='Perfect Prediction', linewidth=4)\n",
        "\n",
        "    # Display R² scores as text annotations\n",
        "    train_r2 = r2_score(y_train, y_train_pred)\n",
        "    test_r2 = r2_score(y_test, y_test_pred)\n",
        "    plt.text(0.05, 0.9, f'Train $R^2$ = {train_r2:.3f}', transform=plt.gca().transAxes,\n",
        "             color='#2408b1', fontsize=22)\n",
        "    plt.text(0.05, 0.83, f'Test $R^2$ = {test_r2:.3f}', transform=plt.gca().transAxes,\n",
        "             color='#ca4016', fontsize=22)\n",
        "\n",
        "    # Set axis labels\n",
        "    plt.xlabel('Actual Resistivity (μΩ⋅cm)', fontsize=22)\n",
        "    plt.ylabel('Predicted Resistivity (μΩ⋅cm)', fontsize=22)\n",
        "\n",
        "    # Configure legend\n",
        "    plt.legend(fontsize=20, framealpha=1)\n",
        "\n",
        "    # Add grid for better readability\n",
        "    plt.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Increase tick label size\n",
        "    plt.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "    # Optimize layout and display\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xfBFYn9p-snK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_train_test_relationship(best_model, X_train, y_train, X_test, y_test, model_name=\"XGBoost\")"
      ],
      "metadata": {
        "id": "Q572AQVWnfwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ FUNCTION 1: Shifted coolwarm ============\n",
        "def shifted_coolwarm_cmap(p, n=256):\n",
        "    \"\"\"\n",
        "    Shift coolwarm colormap so position p becomes neutral center.\n",
        "\n",
        "    \"\"\"\n",
        "    base = plt.get_cmap(\"coolwarm\")\n",
        "    xs = np.linspace(0, 1, n)\n",
        "\n",
        "    cols = []\n",
        "    for x in xs:\n",
        "        if x <= p:\n",
        "            t = 0.5 * (x / p) if p > 0 else 0.0\n",
        "        else:\n",
        "            t = 0.5 + 0.5 * ((x - p) / (1 - p)) if p < 1 else 1.0\n",
        "        cols.append(base(t))\n",
        "\n",
        "    return mcolors.ListedColormap(cols, name=\"coolwarm_shifted\")\n",
        "\n",
        "\n",
        "# ============ FUNCTION 2: Normalization + ticks + colormap ============\n",
        "def make_coolwarm_saturated_with_center(values, vcenter=0.0, nbins=6, steps=(1, 2, 2.5, 5, 10)):\n",
        "    \"\"\"\n",
        "    Create norm, cmap, and ticks for SHAP visualization with saturated colors and zero at neutral.\n",
        "\n",
        "    \"\"\"\n",
        "    x = np.asarray(values, dtype=float)\n",
        "    x = x[np.isfinite(x)]\n",
        "\n",
        "    # Fallback for empty data\n",
        "    if x.size == 0:\n",
        "        norm = mcolors.Normalize(vmin=-1, vmax=1, clip=True)\n",
        "        cmap = plt.get_cmap(\"coolwarm\")\n",
        "        ticks = np.array([-1, 0, 1], dtype=float)\n",
        "        return norm, cmap, ticks\n",
        "\n",
        "    data_min, data_max = float(np.min(x)), float(np.max(x))\n",
        "\n",
        "    # Normalize by actual data range for saturated colors\n",
        "    norm = mcolors.Normalize(vmin=data_min, vmax=data_max, clip=True)\n",
        "\n",
        "    # Generate scientific ticks\n",
        "    loc = mticker.MaxNLocator(nbins=nbins, steps=list(steps))\n",
        "    ticks = loc.tick_values(data_min, data_max)\n",
        "\n",
        "    # Clip ticks to norm range\n",
        "    ticks = ticks[(ticks >= norm.vmin) & (ticks <= norm.vmax)]\n",
        "\n",
        "    # Shift colormap to place vcenter at neutral\n",
        "    if data_min < vcenter < data_max:\n",
        "        p = (vcenter - data_min) / (data_max - data_min)\n",
        "        cmap = shifted_coolwarm_cmap(p, n=256)\n",
        "\n",
        "        # Ensure vcenter is in ticks\n",
        "        if not np.any(np.isclose(ticks, vcenter)):\n",
        "            ticks = np.sort(np.append(ticks, vcenter))\n",
        "    else:\n",
        "        # Use standard coolwarm if vcenter is outside range\n",
        "        cmap = plt.get_cmap(\"coolwarm\")\n",
        "\n",
        "    return norm, cmap, ticks"
      ],
      "metadata": {
        "id": "lRfILiydFgpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ EXTRACT SHAP FROM XGBOOST (DMatrix) ============\n",
        "# Create DMatrix with feature names\n",
        "dmatrix_train = xgb.DMatrix(X_train, feature_names=X_train.columns.tolist())\n",
        "\n",
        "# Get SHAP values via XGBoost native API\n",
        "shap_values_raw = best_model.get_booster().predict(dmatrix_train, pred_contribs=True)\n",
        "shap_values_matrix = shap_values_raw[:, :-1]  # Exclude base value column\n",
        "expected_value = float(shap_values_raw[0, -1])  # Extract base value\n",
        "\n",
        "# Create SHAP Explanation object\n",
        "explanation = shap.Explanation(\n",
        "    values=shap_values_matrix,\n",
        "    base_values=np.full(len(X_train), expected_value),\n",
        "    data=X_train.values,\n",
        "    feature_names=X_train.columns.tolist()\n",
        ")\n",
        "\n",
        "print(f\"Shape SHAP values: {shap_values_matrix.shape}\")\n",
        "print(f\"Expected value: {expected_value}\")"
      ],
      "metadata": {
        "id": "xOVzsZrlFhvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ DEFINE FEATURE ORDER ============\n",
        "desired_order = ['Fe', 'Si', 'Al']\n",
        "\n",
        "# ============ BUILD GRID-SHAP PLOTS ============\n",
        "plt.style.use('default')\n",
        "fig = plt.figure(figsize=(3.5, 10))\n",
        "gs = GridSpec(3, 1, figure=fig, hspace=0.3)\n",
        "\n",
        "for i, feature_name in enumerate(desired_order):\n",
        "    # Get feature index\n",
        "    try:\n",
        "        feature_index = X_train.columns.get_loc(feature_name)\n",
        "    except KeyError:\n",
        "        print(f\"Error: Feature '{feature_name}' not found in X_train.\")\n",
        "        continue\n",
        "\n",
        "    ax = fig.add_subplot(gs[i, 0])\n",
        "    ax.set_facecolor('#f0f0f0')\n",
        "\n",
        "    # Extract SHAP values for current feature\n",
        "    shap_col = explanation[:, feature_index].values\n",
        "\n",
        "    # Get normalized colormap with zero at neutral\n",
        "    norm, cmap_local, ticks = make_coolwarm_saturated_with_center(\n",
        "        shap_col,\n",
        "        vcenter=0.0,\n",
        "        nbins=6\n",
        "    )\n",
        "\n",
        "    # Scatter plot\n",
        "    sc = ax.scatter(\n",
        "        X_train[feature_name],\n",
        "        y_train,\n",
        "        c=shap_col,\n",
        "        cmap=cmap_local,\n",
        "        norm=norm,\n",
        "        alpha=0.9,\n",
        "        s=50,\n",
        "        edgecolor='black',\n",
        "        linewidth=0.3,\n",
        "        marker='o'\n",
        "    )\n",
        "\n",
        "    # Add colorbar\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes('right', size='4.5%', pad=0.05)\n",
        "    cbar = fig.colorbar(sc, cax=cax)\n",
        "\n",
        "    # Set scientific ticks\n",
        "    cbar.set_ticks(ticks)\n",
        "    cbar.formatter = mticker.ScalarFormatter(useMathText=False)\n",
        "    cbar.formatter.set_powerlimits((-2, 3))\n",
        "    cbar.update_ticks()\n",
        "    cbar.ax.tick_params(labelsize=11)\n",
        "    cbar.set_label('SHAP Value', fontsize=12, labelpad=5)\n",
        "\n",
        "    # Configure axes\n",
        "    ax.yaxis.set_major_locator(mticker.MaxNLocator(nbins=5, integer=True))\n",
        "    ax.xaxis.set_major_locator(mticker.MaxNLocator(min_n_ticks=3, nbins=4, integer=True))\n",
        "    ax.set_xlabel(feature_name, fontsize=12, labelpad=4)\n",
        "    ax.set_ylabel(\"Saturation Polarization (T)\", fontsize=12, labelpad=4)\n",
        "    ax.tick_params(axis='both', labelsize=11, pad=4)\n",
        "    ax.grid(True, linestyle=':', alpha=0.35, linewidth=0.45)\n",
        "\n",
        "# Save figure\n",
        "plt.savefig('Composition_Grid-SHAP_rho_XGB.png',\n",
        "            dpi=600,\n",
        "            bbox_inches='tight',\n",
        "            facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "# Download file (Colab only)\n",
        "try:\n",
        "    files.download('Composition_Grid-SHAP_rho_XGB.png')\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "kPv5Z89BFrA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Results**"
      ],
      "metadata": {
        "id": "0L8Hqqh5wFLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After evaluating all models, add this to display and save results:\n",
        "def display_model_results(results_df):\n",
        "    \"\"\"Display and save model comparison results\"\"\"\n",
        "    # Format display\n",
        "    pd.options.display.float_format = '{:.4f}'.format\n",
        "    results_df.set_index('Model', inplace=True)\n",
        "\n",
        "    print(\"\\nFinal Model Comparison:\")\n",
        "    print(\"=\"*60)\n",
        "    print(results_df)\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Highlight best scores\n",
        "    def highlight_metrics(s):\n",
        "        metrics = {'MAE': 'min', 'MSE': 'min', 'RMSE': 'min', 'R²': 'max'}\n",
        "        styles = []\n",
        "        for v in s:\n",
        "            if s.name in metrics:\n",
        "                if metrics[s.name] == 'min' and v == s.min():\n",
        "                    styles.append('background-color: yellow')\n",
        "                elif metrics[s.name] == 'max' and v == s.max():\n",
        "                    styles.append('background-color: lightgreen')\n",
        "                else:\n",
        "                    styles.append('')\n",
        "            else:\n",
        "                styles.append('')\n",
        "        return styles\n",
        "\n",
        "    styled_df = results_df.style.apply(highlight_metrics)\n",
        "    display(styled_df)\n",
        "\n",
        "    # Save to Excel\n",
        "    results_df.to_excel('Composition_rho_metrics.xlsx')\n",
        "    print(\"\\nResults saved to 'Composition_rho_metrics.xlsx'\")\n",
        "\n",
        "\n",
        "# Call this after evaluating all models\n",
        "display_model_results(model_results)\n",
        "\n",
        "# Download file (Colab only)\n",
        "try:\n",
        "    files.download('Composition_rho_metrics.xlsx')\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "6_OqZ3uOwLks"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}