{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qG9G8IZ2rzZ2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Optuna for Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    !pip install -q optuna\n",
        "except:\n",
        "    pass  # For local use: install via pip install optuna or requirements.txt\n",
        "\n",
        "import optuna"
      ],
      "metadata": {
        "id": "Ad5uaVNssrCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CONFIGURATION ======\n",
        "PROPERTY_SELECTION = 1  # 1: Hc (Coercivity) | 2: Js (Saturation Polarization) | 3: rho (Resistivity)\n",
        "RUN_OPTIMIZATION = False  # True: run hyperparameter tuning | False: use pre-optimized parameters\n",
        "\n",
        "# Set property name based on selection\n",
        "if PROPERTY_SELECTION == 1:\n",
        "    property_name = 'Hc'\n",
        "elif PROPERTY_SELECTION == 2:\n",
        "    property_name = 'Js'\n",
        "elif PROPERTY_SELECTION == 3:\n",
        "    property_name = 'rho'\n",
        "\n",
        "# Optuna configuration for hyperparameter tuning\n",
        "OPTUNA_CONFIG = {\n",
        "    'optimization': {'n_trials': 500, 'timeout': None},      # For hyperparameter tuning\n",
        "    'feature_selection': {'n_trials': None, 'timeout':1200 }   # For feature selection\n",
        "}"
      ],
      "metadata": {
        "id": "cDha8mcu6vA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== LOAD DATA ======\n",
        "try:\n",
        "    # For Colab: upload file\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    filename = next(iter(uploaded))\n",
        "    df = pd.read_csv(filename)\n",
        "except:\n",
        "    # For local execution: use predefined path based on property selection\n",
        "    if PROPERTY_SELECTION == 1:\n",
        "        df = pd.read_csv('../data/raw/oliynyk_Hc_raw.csv')\n",
        "    elif PROPERTY_SELECTION == 2:\n",
        "        df = pd.read_csv('../data/raw/oliynyk_Js_raw.csv')\n",
        "    elif PROPERTY_SELECTION == 3:\n",
        "        df = pd.read_csv('../data/raw/oliynyk_rho_raw.csv')\n",
        "\n",
        "print(f\"Loaded {len(df)} samples with {len(df.columns)} columns\")\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "Xtnc1vyxs3pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features (all columns except last) and target variable\n",
        "df_drop_column = df.iloc[:, :-1]\n",
        "target = df.loc[:, 'target']"
      ],
      "metadata": {
        "id": "MgyXmDEdtKYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean feature names by replacing special characters with underscores\n",
        "# LightGBM and some ML libraries don't accept [, ], < in column names\n",
        "import re\n",
        "\n",
        "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
        "\n",
        "df_drop_column.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in df_drop_column.columns.values]"
      ],
      "metadata": {
        "id": "T9uVFHcZtQ_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get features after cleaning\n",
        "features=df_drop_column"
      ],
      "metadata": {
        "id": "jl9np6U1I5JV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle dataset to ensure random distribution\n",
        "features, target = shuffle(features, target, random_state=42)\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "I9T_hKOavLHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# LIGHTGBM HYPERPARAMETER TUNING FOR FEATURE SELECTION\n",
        "# ================================================================================\n",
        "\n",
        "def objective_tune_lgbm(trial):\n",
        "    \"\"\"\n",
        "    Objective function for Optuna hyperparameter optimization.\n",
        "\n",
        "    Suggests hyperparameters from predefined ranges and evaluates model\n",
        "    performance using cross-validation. Returns mean R² score across folds.\n",
        "    \"\"\"\n",
        "\n",
        "    # ====================================================================\n",
        "    # HYPERPARAMETER SEARCH SPACE\n",
        "    # Insert your search ranges below\n",
        "    # ====================================================================\n",
        "\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'r2',\n",
        "        'verbosity': -1,\n",
        "\n",
        "        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart', 'goss']),\n",
        "\n",
        "        # Number of boosting iterations\n",
        "        'n_estimators': trial.suggest_int('n_estimators', MIN_ESTIMATORS, MAX_ESTIMATORS),\n",
        "\n",
        "        # Learning rate (log-uniform)\n",
        "        'learning_rate': trial.suggest_float('learning_rate', MIN_LR, MAX_LR, log=True),\n",
        "\n",
        "        # Maximum leaves per tree\n",
        "        'num_leaves': trial.suggest_int('num_leaves', MIN_LEAVES, MAX_LEAVES),\n",
        "\n",
        "        # Maximum tree depth\n",
        "        'max_depth': trial.suggest_int('max_depth', MIN_DEPTH, MAX_DEPTH),\n",
        "\n",
        "        # Minimum samples per leaf\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', MIN_SAMPLES, MAX_SAMPLES),\n",
        "\n",
        "        # Feature fraction per tree\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', MIN_COLSAMPLE, MAX_COLSAMPLE),\n",
        "\n",
        "        # L1 regularization (log-uniform)\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', MIN_ALPHA, MAX_ALPHA, log=True),\n",
        "\n",
        "        # L2 regularization (log-uniform)\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', MIN_LAMBDA, MAX_LAMBDA, log=True),\n",
        "    }\n",
        "\n",
        "    # Add subsample parameters (not compatible with goss)\n",
        "    if param['boosting_type'] != 'goss':\n",
        "        # Row subsampling fraction\n",
        "        param['subsample'] = trial.suggest_float('subsample', MIN_SUBSAMPLE, MAX_SUBSAMPLE)\n",
        "        # Bagging frequency\n",
        "        param['bagging_freq'] = trial.suggest_int('bagging_freq', MIN_FREQ, MAX_FREQ)\n",
        "\n",
        "    # Train model with suggested hyperparameters\n",
        "    model = lgb.LGBMRegressor(random_state=42, **param)\n",
        "\n",
        "    # Evaluate using 5-fold cross-validation\n",
        "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='r2')\n",
        "\n",
        "    # Return mean R² score across all folds\n",
        "    return np.mean(scores)"
      ],
      "metadata": {
        "id": "20Z_Adyq4czd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== RUN OPTIMIZATION OR USE PRE-OPTIMIZED PARAMETERS ======\n",
        "if RUN_OPTIMIZATION:\n",
        "    print(\"Running hyperparameter optimization...\")\n",
        "    study_lgbm = optuna.create_study(direction='maximize')\n",
        "\n",
        "    # Extract config and filter out None values\n",
        "    config = OPTUNA_CONFIG['optimization']\n",
        "    optimize_kwargs = {k: v for k, v in config.items() if v is not None}\n",
        "\n",
        "    study_lgbm.optimize(objective_tune_lgbm, **optimize_kwargs)\n",
        "\n",
        "    # Display results\n",
        "    print('\\nBest trial results:')\n",
        "    trial = study_lgbm.best_trial\n",
        "    print(f'R² score: {trial.value:.4f}\\n')\n",
        "\n",
        "    print('Best parameters:')\n",
        "    print('best_lgbm_params = {')\n",
        "    for key, value in trial.params.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"    '{key}': {value},\")\n",
        "        elif isinstance(value, str):\n",
        "            print(f\"    '{key}': '{value}',\")\n",
        "        else:\n",
        "            print(f\"    '{key}': {value},\")\n",
        "    print('}\\n')\n",
        "\n",
        "    best_lgbm_params = study_lgbm.best_params\n",
        "\n",
        "# ====== PRE-OPTIMIZED PARAMETERS ======\n",
        "else:\n",
        "    # Insert your optimized parameters below\n",
        "    print(\"Using pre-optimized hyperparameters...\")\n",
        "\n",
        "    best_lgbm_params = {\n",
        "        # ====== REPLACE WITH YOUR OPTIMIZED PARAMETERS ======\n",
        "        'boosting_type': 'gbdt',           # gbdt, dart, or goss\n",
        "        'n_estimators': 100,                # Number of iterations\n",
        "        'learning_rate': 0.01,              # Step size\n",
        "        'num_leaves': 31,                   # Max leaves per tree\n",
        "        'max_depth': 10,                    # Tree depth\n",
        "        'min_child_samples': 10,            # Min samples per leaf\n",
        "        'colsample_bytree': 0.8,            # Feature fraction\n",
        "        'reg_alpha': 0.1,                   # L1 regularization\n",
        "        'reg_lambda': 0.1,                  # L2 regularization\n",
        "        # 'subsample': 0.8,                 # Row subsampling (gbdt/dart only)\n",
        "        # 'bagging_freq': 5,                # Bagging frequency (gbdt/dart only)\n",
        "        # ====================================================\n",
        "    }\n",
        "\n",
        "    print(\"Parameters:\")\n",
        "    for key, value in best_lgbm_params.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "IQun5eg37mXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# FEATURE SELECTION USING OPTUNA\n",
        "# ================================================================================\n",
        "# Uses LightGBM with optimized hyperparameters to select the best feature subset.\n",
        "# Features are selected individually (True/False) to maximize R² with a penalty\n",
        "# for deviating from the target number of features (10).\n",
        "# ================================================================================\n",
        "\n",
        "\n",
        "def objective_feature_selection(trial):\n",
        "    \"\"\"\n",
        "    Objective function for feature selection optimization.\n",
        "\n",
        "    For each feature, suggests whether to include it (True/False).\n",
        "    Evaluates model performance on selected features using cross-validation.\n",
        "    Applies penalty for feature count deviation from target (10 features).\n",
        "\n",
        "    Returns: R² score minus penalty\n",
        "    \"\"\"\n",
        "\n",
        "    # Select features by suggesting True/False for each feature\n",
        "    selected_features = []\n",
        "    for feature_name in X_train.columns:\n",
        "        if trial.suggest_categorical(feature_name, [True, False]):\n",
        "            selected_features.append(feature_name)\n",
        "\n",
        "    # Return worst score if no features selected\n",
        "    if not selected_features:\n",
        "        return -np.inf\n",
        "\n",
        "    # Train model on selected features only\n",
        "    X_selected = X_train[selected_features]\n",
        "\n",
        "    # Use LightGBM with pre-optimized hyperparameters\n",
        "    model = lgb.LGBMRegressor(random_state=42, **best_lgbm_params)\n",
        "\n",
        "    # Evaluate using 5-fold cross-validation\n",
        "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    scores = cross_val_score(model, X_selected, y_train, cv=cv, scoring='r2')\n",
        "    r2_score = np.mean(scores)\n",
        "\n",
        "\n",
        "    # Apply penalty: 0.025 for each feature away from target (10 features)\n",
        "    # Adjust penalty if optimization struggles (e.g., if R² is negative, reduce penalty to 0.01 or 0.005)\n",
        "    penalty = 0.025 * abs(len(selected_features) - 10)\n",
        "\n",
        "    return r2_score - penalty"
      ],
      "metadata": {
        "id": "H4tY1anZ-tbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== RUN FEATURE SELECTION ======\n",
        "print(\"Running feature selection optimization...\")\n",
        "\n",
        "# Create Optuna study to maximize (R² - penalty)\n",
        "study_fs = optuna.create_study(direction='maximize')\n",
        "\n",
        "# Extract config and filter out None values\n",
        "config = OPTUNA_CONFIG['feature_selection']\n",
        "optimize_kwargs = {k: v for k, v in config.items() if v is not None}\n",
        "\n",
        "# Run feature selection optimization\n",
        "study_fs.optimize(objective_feature_selection, **optimize_kwargs)"
      ],
      "metadata": {
        "id": "nikg1da8-0YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== DISPLAY FEATURE SELECTION RESULTS ======\n",
        "best_trial = study_fs.best_trial\n",
        "\n",
        "# Extract feature names where Optuna selected True\n",
        "selected_features = [name for name, selected in best_trial.params.items() if selected]\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FEATURE SELECTION COMPLETED\")\n",
        "print(f\"Objective value (R² - penalty): {best_trial.value:.4f}\")\n",
        "print(f\"Number of selected features: {len(selected_features)}\")\n",
        "print(\"\\nSelected features:\")\n",
        "# Print each feature name on a separate line (sorted alphabetically)\n",
        "for feature in sorted(selected_features):\n",
        "    print(f\"  - {feature}\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "-A6FC7BuiewK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== SAVE RESULTS ======\n",
        "# Create new dataset with selected features only\n",
        "new_dataset = df_drop_column[selected_features].copy()\n",
        "print(\"\\nCreated new dataset 'new_dataset' with shape:\", new_dataset.shape)\n",
        "\n",
        "# Combine selected features with target column\n",
        "df_to_download = pd.concat([new_dataset, target], axis=1)\n",
        "\n",
        "# Save to CSV and download\n",
        "df_to_download.to_csv(f'Oliynyk_{property_name}_optunaFS.csv', index=False)\n",
        "files.download(f'Oliynyk_{property_name}_optunaFS.csv')"
      ],
      "metadata": {
        "id": "Geq5z2W_gOjC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}